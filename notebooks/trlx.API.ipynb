{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95db1950",
   "metadata": {},
   "source": [
    "# Learn trlX in 60 Minutes - API Tutorial\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "**trlX** (Transformer Reinforcement Learning X) is a distributed training framework designed to fine-tune Large Language Models (LLMs) using Reinforcement Learning (RL). It is particularly known for scaling RLHF (Reinforcement Learning from Human Feedback) to large models.\n",
    "\n",
    "In this tutorial, we will explore the core API of `trlx` by setting up a simple **PPO (Proximal Policy Optimization)** training loop. We will also demonstrate how to use **DPO (Direct Preference Optimization)** using the custom extensions in this project.\n",
    "\n",
    "**Goal**: Understand how to configure, train, and use a model with `trlx`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4deefd",
   "metadata": {},
   "source": [
    "## 2. Setup and Installation\n",
    "\n",
    "First, ensure `trlx` and its dependencies are installed. If you are running this in a new environment, uncomment the line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89687b31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T01:39:16.744456Z",
     "iopub.status.busy": "2025-12-07T01:39:16.743924Z",
     "iopub.status.idle": "2025-12-07T01:39:19.184162Z",
     "shell.execute_reply": "2025-12-07T01:39:19.183563Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1206 21:22:40.986000 50375 torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n",
      "/Users/chitturi/Downloads/RLHF_News_Summarization_System/venv/lib/python3.10/site-packages/ray/_private/parameter.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "WARNING: Training on CPU will be extremely slow. This tutorial is best run with a GPU.\n"
     ]
    }
   ],
   "source": [
    "# !pip install trlx\n",
    "\n",
    "import trlx\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Check for GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == \"cpu\":\n",
    "    print(\"WARNING: Training on CPU will be extremely slow. This tutorial is best run with a GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8775f8cd",
   "metadata": {},
   "source": [
    "## 3. Configuration (`TRLConfig`)\n",
    "\n",
    "`trlx` uses a configuration object to manage the many hyperparameters involved in RL training. The `TRLConfig` object controls:\n",
    "- **Model**: Which model to load (e.g., `gpt2`).\n",
    "- **Train**: Batch size, sequence length, epochs.\n",
    "- **Method**: Algorithm-specific settings (e.g., PPO clip range, chunk size).\n",
    "\n",
    "We will manually construct the configuration to ensure full control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63dfac25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T01:39:19.185355Z",
     "iopub.status.busy": "2025-12-07T01:39:19.185179Z",
     "iopub.status.idle": "2025-12-07T01:39:19.189054Z",
     "shell.execute_reply": "2025-12-07T01:39:19.188600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration ready.\n"
     ]
    }
   ],
   "source": [
    "from trlx.data.configs import TRLConfig, TrainConfig, ModelConfig, OptimizerConfig, SchedulerConfig, TokenizerConfig\n",
    "from trlx.models.modeling_ppo import PPOConfig\n",
    "\n",
    "# 1. Model Settings\n",
    "model_config = ModelConfig(\n",
    "    model_path=\"gpt2\",  # Using GPT-2 (small) for demonstration\n",
    "    model_arch_type=\"causal\"\n",
    ")\n",
    "\n",
    "# 2. Tokenizer Settings\n",
    "tokenizer_config = TokenizerConfig(\n",
    "    tokenizer_path=\"gpt2\",\n",
    "    padding_side=\"left\"\n",
    ")\n",
    "\n",
    "# 3. Training Settings\n",
    "train_config = TrainConfig(\n",
    "    total_steps=10,\n",
    "    seq_length=128,\n",
    "    epochs=1,\n",
    "    batch_size=4,\n",
    "    checkpoint_interval=100,\n",
    "    eval_interval=100,\n",
    "    pipeline=\"PPOPipeline\",\n",
    "    trainer=\"AcceleratePPOTrainer\"\n",
    ")\n",
    "\n",
    "# 4. PPO Method Settings\n",
    "method_config = PPOConfig(\n",
    "    name=\"PPOConfig\",\n",
    "    num_rollouts=128,\n",
    "    chunk_size=4,\n",
    "    ppo_epochs=1,\n",
    "    init_kl_coef=0.1,\n",
    "    target=6.0,\n",
    "    horizon=10000,\n",
    "    gamma=1.0,\n",
    "    lam=0.95,\n",
    "    cliprange=0.2,\n",
    "    cliprange_value=0.2,\n",
    "    vf_coef=1.0,\n",
    "    scale_reward=\"ignored\",\n",
    "    ref_mean=None,\n",
    "    ref_std=None,\n",
    "    cliprange_reward=10.0,\n",
    "    gen_kwargs={\"max_new_tokens\": 40}\n",
    ")\n",
    "\n",
    "# 5. Optimizer & Scheduler\n",
    "optimizer_config = OptimizerConfig(name=\"adamw\", kwargs={\"lr\": 1.0e-5, \"betas\": [0.9, 0.95], \"eps\": 1.0e-8, \"weight_decay\": 1.0e-6})\n",
    "scheduler_config = SchedulerConfig(name=\"cosine_annealing\", kwargs={\"T_max\": 10000, \"eta_min\": 1.0e-5})\n",
    "\n",
    "# Combine into TRLConfig\n",
    "config = TRLConfig(\n",
    "    model=model_config,\n",
    "    tokenizer=tokenizer_config,\n",
    "    train=train_config,\n",
    "    method=method_config,\n",
    "    optimizer=optimizer_config,\n",
    "    scheduler=scheduler_config\n",
    ")\n",
    "\n",
    "print(\"Configuration ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca251692",
   "metadata": {},
   "source": [
    "## 4. The Reward Function\n",
    "\n",
    "In Reinforcement Learning from Human Feedback (RLHF), the **Reward Function** is the critic. It evaluates the text generated by the model and assigns a score.\n",
    "\n",
    "For this tutorial, we will define a simple **heuristic reward function**. We want the model to generate text containing the word **\"cat\"**.\n",
    "\n",
    "- **Input**: A list of generated strings.\n",
    "- **Output**: A list of scalar rewards (floats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77ca06f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T01:39:19.190056Z",
     "iopub.status.busy": "2025-12-07T01:39:19.189968Z",
     "iopub.status.idle": "2025-12-07T01:39:19.193138Z",
     "shell.execute_reply": "2025-12-07T01:39:19.192691Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Rewards: [1.0, 0.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "def reward_fn(samples, **kwargs):\n",
    "    \"\"\"\n",
    "    Assigns a reward based on the number of times 'cat' appears in the generated text.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for sample in samples:\n",
    "        # Calculate reward: +1.0 for every mention of 'cat'\n",
    "        count = sample.lower().count(\"cat\")\n",
    "        rewards.append(float(count))\n",
    "    return rewards\n",
    "\n",
    "# Test the reward function\n",
    "test_samples = [\"I love my cat.\", \"Dogs are great.\", \"The cat sat on the cat.\"]\n",
    "print(f\"Test Rewards: {reward_fn(test_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daabb6b9",
   "metadata": {},
   "source": [
    "## 5. The Training Loop (`trlx.train`)\n",
    "\n",
    "The `trlx.train` function is the main entry point. It handles:\n",
    "1.  Loading the model and tokenizer.\n",
    "2.  Generating samples (rollouts).\n",
    "3.  Calculating rewards using your function.\n",
    "4.  Updating the model using PPO.\n",
    "\n",
    "We provide a list of **prompts** to kickstart the generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29062ab5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T01:39:19.193980Z",
     "iopub.status.busy": "2025-12-07T01:39:19.193899Z",
     "iopub.status.idle": "2025-12-07T01:39:19.197205Z",
     "shell.execute_reply": "2025-12-07T01:39:19.196827Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping actual training call because no GPU is detected.\n",
      "In a GPU environment, 'trlx.train' would execute the PPO loop.\n"
     ]
    }
   ],
   "source": [
    "# Prompts to start generation\n",
    "prompts = [\n",
    "    \"My favorite animal is\",\n",
    "    \"The quick brown fox\",\n",
    "    \"I saw a\",\n",
    "    \"Once upon a time\",\n",
    "    \"The pet store had\"\n",
    "]\n",
    "\n",
    "# Evaluation prompts (to see progress)\n",
    "eval_prompts = [\"My favorite animal is\"]\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Starting training... (This may take a few minutes)\")\n",
    "    trainer = trlx.train(\n",
    "        reward_fn=reward_fn,\n",
    "        prompts=prompts,\n",
    "        eval_prompts=eval_prompts,\n",
    "        config=config\n",
    "    )\n",
    "    print(\"Training complete!\")\n",
    "else:\n",
    "    print(\"Skipping actual training call because no GPU is detected.\")\n",
    "    print(\"In a GPU environment, 'trlx.train' would execute the PPO loop.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf13bb8",
   "metadata": {},
   "source": [
    "## 6. Inference and Verification\n",
    "\n",
    "Once trained, the `trainer` object wraps the fine-tuned model. We can use it to generate text and verify if it learned the policy (to mention \"cat\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00a61af9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T01:39:19.197990Z",
     "iopub.status.busy": "2025-12-07T01:39:19.197913Z",
     "iopub.status.idle": "2025-12-07T01:39:19.201146Z",
     "shell.execute_reply": "2025-12-07T01:39:19.200743Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping inference.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Generate text using the fine-tuned model\n",
    "    output = trainer.generate(prompts=[\"My favorite animal is\"], length=20)\n",
    "    print(\"Generated:\", output)\n",
    "    \n",
    "    # Check if it learned\n",
    "    if \"cat\" in output[0].lower():\n",
    "        print(\"Success! The model mentioned 'cat'.\")\n",
    "    else:\n",
    "        print(\"The model didn't mention 'cat'. It might need more training steps.\")\n",
    "else:\n",
    "    print(\"Skipping inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132a7734",
   "metadata": {},
   "source": [
    "## 7. Advanced: Direct Preference Optimization (DPO)\n",
    "\n",
    "This project extends `trlx` to support DPO, which is a more stable alternative to PPO. Instead of a reward function, DPO uses a dataset of preferred and rejected responses.\n",
    "\n",
    "Below is an example of how to configure and run DPO using the custom `AccelerateDPOTrainer` included in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "091df7c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-07T01:39:19.201940Z",
     "iopub.status.busy": "2025-12-07T01:39:19.201866Z",
     "iopub.status.idle": "2025-12-07T01:39:19.213214Z",
     "shell.execute_reply": "2025-12-07T01:39:19.212711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPO Configuration loaded successfully.\n",
      "To run DPO, you would call trlx.train() with this config and a preference dataset.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from trlx_custom.trainer.accelerate_dpo_trainer import DPOConfig\n",
    "    \n",
    "    # Initialize DPO Configuration\n",
    "    dpo_config = DPOConfig(\n",
    "        beta=0.1,  # The beta parameter controls the strength of the KL penalty\n",
    "        gen_kwargs={\"max_new_tokens\": 64}\n",
    "    )\n",
    "    \n",
    "    # In a real scenario, you would pass a dataset with 'chosen' and 'rejected' columns\n",
    "    # trainer = trlx.train(\n",
    "    #     model_path=\"gpt2\",\n",
    "    #     config=dpo_config,\n",
    "    #     samples=...,\n",
    "    #     rewards=... # In DPO, rewards are implicit in the preference pairs\n",
    "    # )\n",
    "    \n",
    "    print(\"DPO Configuration loaded successfully.\")\n",
    "    print(\"To run DPO, you would call trlx.train() with this config and a preference dataset.\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"Could not import DPOConfig. Ensure you are running this notebook from the project root.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2bcf8a",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "In this tutorial, we covered:\n",
    "1.  **Configuration**: How to set up `TRLConfig` for PPO.\n",
    "2.  **Reward Function**: How to define a custom Python function to guide the model.\n",
    "3.  **Training**: How to launch the training loop with `trlx.train`.\n",
    "4.  **DPO**: How to configure the custom Direct Preference Optimization trainer.\n",
    "\n",
    "This is the foundation of RLHF. In real-world scenarios, the simple `reward_fn` is replaced by a **Reward Model** (another neural network) trained on human preferences, as seen in the main project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
